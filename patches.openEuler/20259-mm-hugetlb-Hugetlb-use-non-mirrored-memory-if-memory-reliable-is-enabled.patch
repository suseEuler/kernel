From 74bfdf157f1fd40b5d7966d1d94fcd690ad93a3e Mon Sep 17 00:00:00 2001
From: Ma Wupeng <mawupeng1@huawei.com>
Date: Fri, 11 Nov 2022 09:32:45 +0800
Subject: [PATCH] mm/hugetlb: Hugetlb use non-mirrored memory if memory
 reliable is enabled
Patch-mainline: Not yet, from openEuler
References: bsn#22
openEuler-commit: 74bfdf157f1fd40b5d7966d1d94fcd690ad93a3e
Modified-by-SEL: No


hulk inclusion
category: feature
bugzilla: https://gitee.com/openeuler/kernel/issues/I4SK3S
CVE: NA

--------------------------------

Previous memory allocation in memblock for hugetlb may use mirrored or
non-mirrored memory depends on the system's memory status. However this is
not suitable if hugetlb user want to alloc memory from non-mirrored memory
if memory reliable is enabled.

In order to solve this problem, hugetlb use MEMBLOCK_NOMIRROR flag to alloc
memory from non-mirrored region without fallback to mirrored region.

Signed-off-by: Ma Wupeng <mawupeng1@huawei.com>
Reviewed-by: Kefeng Wang <wangkefeng.wang@huawei.com>
Signed-off-by: Guoqing Jiang <guoqing.jiang@suse.com>
---
 mm/hugetlb.c | 18 ++++++++++++++++--
 1 file changed, 16 insertions(+), 2 deletions(-)

diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index f5f8227b090b..d687f271901e 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -2697,6 +2697,20 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,
 	return ERR_PTR(-ENOSPC);
 }
 
+static void *__init __alloc_bootmem_huge_page_inner(phys_addr_t size,
+						    phys_addr_t align,
+						    phys_addr_t min_addr,
+						    phys_addr_t max_addr,
+						    int nid)
+{
+	if (!mem_reliable_is_enabled())
+		return memblock_alloc_try_nid_raw(size, align, max_addr,
+						  max_addr, nid);
+
+	return memblock_alloc_try_nid_raw_flags(size, align, max_addr, max_addr,
+						nid, MEMBLOCK_NOMIRROR);
+}
+
 int alloc_bootmem_huge_page(struct hstate *h, int nid)
 	__attribute__ ((weak, alias("__alloc_bootmem_huge_page")));
 int __alloc_bootmem_huge_page(struct hstate *h, int nid)
@@ -2712,7 +2726,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)
 
 	/* do node specific alloc */
 	if (nid != NUMA_NO_NODE) {
-		m = memblock_alloc_try_nid_raw(huge_page_size(h), huge_page_size(h),
+		m = __alloc_bootmem_huge_page_inner(huge_page_size(h), huge_page_size(h),
 				0, MEMBLOCK_ALLOC_ACCESSIBLE, nid);
 		if (!m)
 			return 0;
@@ -2720,7 +2734,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)
 	}
 	/* allocate from next node when distributing huge pages */
 	for_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {
-		m = memblock_alloc_try_nid_raw(
+		m = __alloc_bootmem_huge_page_inner(
 				huge_page_size(h), huge_page_size(h),
 				0, MEMBLOCK_ALLOC_ACCESSIBLE, node);
 		/*
-- 
2.33.0

