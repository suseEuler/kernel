From 73f82da4b1c736e3c2b7983cce17216c4a563466 Mon Sep 17 00:00:00 2001
From: Zhou Guanghui <zhouguanghui1@huawei.com>
Date: Tue, 29 Nov 2022 13:57:33 +0000
Subject: [PATCH] Hugtlb: bugfix for hugetlb remap
Patch-mainline: Not yet, from openEuler
References: bsn#22
openEuler-commit: 73f82da4b1c736e3c2b7983cce17216c4a563466
Modified-by-SEL: No


hulk inclusion
category: bugfix
bugzilla: https://gitee.com/openeuler/kernel/issues/I63SDZ

-------------------------------

In Ascend feature, some driver's VMA struct contains hugepage and nomal
page. In this case, we can's using VMA's message to alloc page. To fix
this, we only alloc huge page directly in this interface rather than
using VMA's message.

Signed-off-by: Zhou Guanghui <zhouguanghui1@huawei.com>
Signed-off-by: Guoqing Jiang <guoqing.jiang@suse.com>
---
 mm/hugetlb.c | 56 ++++++++++++++++++++++++++--------------------------
 1 file changed, 28 insertions(+), 28 deletions(-)

diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 6b96eda50977..bb97bbaac65a 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -6283,39 +6283,43 @@ struct page *hugetlb_alloc_hugepage(int nid, int flag)
 }
 EXPORT_SYMBOL_GPL(hugetlb_alloc_hugepage);
 
+static pte_t *hugetlb_huge_pte_alloc(struct mm_struct *mm, unsigned long addr,
+				     unsigned long size)
+{
+	pgd_t *pgdp;
+	p4d_t *p4dp;
+	pud_t *pudp;
+	pte_t *ptep = NULL;
+
+	pgdp = pgd_offset(mm, addr);
+	p4dp = p4d_offset(pgdp, addr);
+	pudp = pud_alloc(mm, p4dp, addr);
+	if (!pudp)
+		return NULL;
+
+	ptep = (pte_t *)pmd_alloc(mm, pudp, addr);
+
+	return ptep;
+}
+
 static int __hugetlb_insert_hugepage(struct mm_struct *mm, unsigned long addr,
-				     pgprot_t prot, unsigned long pfn, bool special)
+				     pgprot_t prot, unsigned long pfn)
 {
 	int ret = 0;
 	pte_t *ptep, entry;
 	struct hstate *h;
-	struct vm_area_struct *vma;
-	struct address_space *mapping;
 	spinlock_t *ptl;
 
 	h = size_to_hstate(PMD_SIZE);
 	if (!h)
 		return -EINVAL;
 
-	if (!IS_ALIGNED(addr, PMD_SIZE))
-		return -EINVAL;
-
-	vma = find_vma(mm, addr);
-	if (!vma || !range_in_vma(vma, addr, addr + PMD_SIZE))
-		return -EINVAL;
-
-	mapping = vma->vm_file->f_mapping;
-	i_mmap_lock_read(mapping);
-	ptep = huge_pte_alloc(mm, addr, huge_page_size(h));
-	if (!ptep) {
-		ret = -ENXIO;
-		goto out_unlock;
-	}
+	ptep = hugetlb_huge_pte_alloc(mm, addr, huge_page_size(h));
+	if (!ptep)
+		return -ENXIO;
 
-	if (WARN_ON(ptep && !pte_none(*ptep) && !pmd_huge(*(pmd_t *)ptep))) {
-		ret = -ENXIO;
-		goto out_unlock;
-	}
+	if (WARN_ON(ptep && !pte_none(*ptep) && !pmd_huge(*(pmd_t *)ptep)))
+		return -ENXIO;
 
 	entry = pfn_pte(pfn, prot);
 	entry = huge_pte_mkdirty(entry);
@@ -6323,31 +6327,27 @@ static int __hugetlb_insert_hugepage(struct mm_struct *mm, unsigned long addr,
 		entry = huge_pte_mkwrite(entry);
 	entry = pte_mkyoung(entry);
 	entry = pte_mkhuge(entry);
-	if (special)
-		entry = pte_mkspecial(entry);
+	entry = pte_mkspecial(entry);
 
 	ptl = huge_pte_lockptr(h, mm, ptep);
 	spin_lock(ptl);
 	set_huge_pte_at(mm, addr, ptep, entry);
 	spin_unlock(ptl);
 
-out_unlock:
-	i_mmap_unlock_read(mapping);
-
 	return ret;
 }
 
 int hugetlb_insert_hugepage_pte(struct mm_struct *mm, unsigned long addr,
 				pgprot_t prot, struct page *hpage)
 {
-	return __hugetlb_insert_hugepage(mm, addr, prot, page_to_pfn(hpage), false);
+	return __hugetlb_insert_hugepage(mm, addr, prot, page_to_pfn(hpage));
 }
 EXPORT_SYMBOL_GPL(hugetlb_insert_hugepage_pte);
 
 int hugetlb_insert_hugepage_pte_by_pa(struct mm_struct *mm, unsigned long addr,
 				      pgprot_t prot, unsigned long phy_addr)
 {
-	return __hugetlb_insert_hugepage(mm, addr, prot, phy_addr >> PAGE_SHIFT, true);
+	return __hugetlb_insert_hugepage(mm, addr, prot, phy_addr >> PAGE_SHIFT);
 }
 EXPORT_SYMBOL_GPL(hugetlb_insert_hugepage_pte_by_pa);
 
-- 
2.33.0

