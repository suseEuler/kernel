From a669dc1962d38bb405431f94ca6d4e30107a2fe1 Mon Sep 17 00:00:00 2001
From: Zhang Qiao <zhangqiao22@huawei.com>
Date: Sat, 28 May 2022 17:57:25 +0800
Subject: [PATCH] sched: Throttle offline task at tracehook_notify_resume()
Patch-mainline: Not yet, from openEuler
References: bsn#22
openEuler-commit: a669dc1962d38bb405431f94ca6d4e30107a2fe1
Modified-by-SEL: No


hulk inclusion
category: bugfix
bugzilla: https://gitee.com/openeuler/kernel/issues/I4VZJT
CVE: NA

--------------------------------

Before, when detect the cpu is overloaded, we throttle offline
tasks at exit_to_user_mode_loop() before returning to user mode.
Some architects(e.g.,arm64) do not support QOS scheduler because
a task do not via exit_to_user_mode_loop() return to userspace at
these platforms.
In order to slove this problem and support qos scheduler on all
architectures, if we require throttling offline tasks, we set flag
TIF_NOTIFY_RESUME to an offline task when it is picked and throttle
it at tracehook_notify_resume().

Signed-off-by: Zhang Qiao <zhangqiao22@huawei.com>
Reviewed-by: Chen Hui <judy.chenhui@huawei.com>
Signed-off-by: Zheng Zengkai <zhengzengkai@huawei.com>
Signed-off-by: Guoqing Jiang <guoqing.jiang@suse.com>
---
 include/linux/tracehook.h |    4 ++++
 kernel/entry/common.c     |    7 +------
 kernel/sched/fair.c       |   34 +++++++++++++++++++++++++++-------
 3 files changed, 32 insertions(+), 13 deletions(-)

--- a/include/linux/tracehook.h
+++ b/include/linux/tracehook.h
@@ -196,6 +196,10 @@ static inline void tracehook_notify_resu
 
 	mem_cgroup_handle_over_high();
 	blkcg_maybe_throttle_current();
+#ifdef CONFIG_QOS_SCHED
+	sched_qos_offline_wait();
+#endif
+
 }
 
 /*
--- a/kernel/entry/common.c
+++ b/kernel/entry/common.c
@@ -169,10 +169,6 @@ static unsigned long exit_to_user_mode_l
 		if (ti_work & (_TIF_SIGPENDING | _TIF_NOTIFY_SIGNAL))
 			handle_signal_work(regs, ti_work);
 
-#ifdef CONFIG_QOS_SCHED
-		sched_qos_offline_wait();
-#endif
-
 		if (ti_work & _TIF_NOTIFY_RESUME) {
 			tracehook_notify_resume(regs);
 			rseq_handle_notify_resume(NULL, regs);
@@ -207,8 +203,7 @@ static void exit_to_user_mode_prepare(st
 	/* Flush pending rcuog wakeup before the last need_resched() check */
 	tick_nohz_user_enter_prepare();
 
-	if (unlikely((ti_work & EXIT_TO_USER_MODE_WORK) ||
-		      sched_qos_cpu_overload()))
+	if (unlikely(ti_work & EXIT_TO_USER_MODE_WORK))
 		ti_work = exit_to_user_mode_loop(regs, ti_work);
 
 	arch_exit_to_user_mode_prepare(regs, ti_work);
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -26,6 +26,7 @@
 #endif
 #ifdef CONFIG_QOS_SCHED
 #include <linux/delay.h>
+#include <linux/tracehook.h>
 #endif
 
 /*
@@ -7198,6 +7199,7 @@ preempt:
 }
 
 #ifdef CONFIG_QOS_SCHED
+
 static void start_qos_hrtimer(int cpu);
 static void throttle_qos_cfs_rq(struct cfs_rq *cfs_rq)
 {
@@ -7362,15 +7364,11 @@ void sched_qos_offline_wait(void)
 		rcu_read_lock();
 		qos_level = task_group(current)->qos_level;
 		rcu_read_unlock();
-		if (qos_level != -1 || signal_pending(current))
+		if (qos_level != -1 || fatal_signal_pending(current))
 			break;
-		msleep_interruptible(sysctl_offline_wait_interval);
-	}
-}
 
-int sched_qos_cpu_overload(void)
-{
-	return __this_cpu_read(qos_cpu_overload);
+		schedule_timeout_killable(msecs_to_jiffies(sysctl_offline_wait_interval));
+	}
 }
 
 static enum hrtimer_restart qos_overload_timer_handler(struct hrtimer *timer)
@@ -7403,6 +7401,23 @@ void init_qos_hrtimer(int cpu)
 	hrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
 	hrtimer->function = qos_overload_timer_handler;
 }
+
+/*
+ * To avoid Priority inversion issues, when this cpu is qos_cpu_overload,
+ * we should schedule offline tasks to run so that they can leave kernel
+ * critical sections, and throttle them before returning to user mode.
+ */
+static void qos_schedule_throttle(struct task_struct *p)
+{
+	if (unlikely(current->flags & PF_KTHREAD))
+		return;
+
+	if (unlikely(this_cpu_read(qos_cpu_overload))) {
+		if (task_group(p)->qos_level < 0)
+			set_notify_resume(p);
+	}
+}
+
 #endif
 
 #ifdef CONFIG_QOS_SCHED_SMT_EXPELLER
@@ -7710,9 +7725,14 @@ done: __maybe_unused;
 
 	update_misfit_status(p, rq);
 
+#ifdef CONFIG_QOS_SCHED
+	qos_schedule_throttle(p);
+#endif
+
 #ifdef CONFIG_QOS_SCHED_SMT_EXPELLER
 	qos_smt_expel(this_cpu, p);
 #endif
+
 	return p;
 
 idle:
